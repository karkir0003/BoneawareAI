{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CHWhyuzyfvZ"
   },
   "source": [
    "# BoneawareAI\n",
    "\n",
    "Authors: Karthik Subramanian, Charles Green, Sai Anurag Pichika, Saarang Prabhuram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFJ0ozK8yp3V"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Extensions\n",
    "\n",
    "Before getting started we need to run some standard code to set up our environment. You'll need to execute this code again each time you start the notebook.\n",
    "\n",
    "First, run this cell to load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This enables us to modify `.py` source files and reintegrate them into the notebook, ensuring a smooth editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17962,
     "status": "ok",
     "timestamp": 1731635628220,
     "user": {
      "displayName": "Karthik Subramanian",
      "userId": "06529395952522016450"
     },
     "user_tz": 300
    },
    "id": "8fFVquhuCVrq",
    "outputId": "f97e5b79-8387-45ca-acbd-b07fe4a2d5df"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1731635709019,
     "user": {
      "displayName": "Karthik Subramanian",
      "userId": "06529395952522016450"
     },
     "user_tz": 300
    },
    "id": "It7_Rq3X4g3z",
    "outputId": "417d2803-8efb-41ed-9e90-851c04ab634e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/BoneawareAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m PROJECT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBoneawareAI\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m GOOGLE_DRIVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROJECT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOOGLE_DRIVE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/MyDrive/BoneawareAI'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_PATH = 'BoneawareAI'\n",
    "GOOGLE_DRIVE_PATH = f'/content/drive/MyDrive/{PROJECT_PATH}'\n",
    "os.chdir(GOOGLE_DRIVE_PATH)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1731636108951,
     "user": {
      "displayName": "Karthik Subramanian",
      "userId": "06529395952522016450"
     },
     "user_tz": 300
    },
    "id": "1w70X5avjfR1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH) # this is important for the imports in the .py files to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21568,
     "status": "ok",
     "timestamp": 1731635706314,
     "user": {
      "displayName": "Karthik Subramanian",
      "userId": "06529395952522016450"
     },
     "user_tz": 300
    },
    "id": "TkDj-6n-EMYa",
    "outputId": "406f0642-1aaf-40e7-9eac-79c5aa12fa97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.4.1 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (5.4.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (1.34.154)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.154 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from boto3) (1.34.154)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from botocore<1.35.0,>=1.34.154->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from botocore<1.35.0,>=1.34.154->boto3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.154->boto3) (1.16.0)\n",
      "Requirement already satisfied: configparser in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (5.0.2)\n",
      "Requirement already satisfied: torch in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\charl\\anaconda3\\envs\\boneaware-ai\\lib\\site-packages (from torch) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml==5.4.1\n",
    "!pip install boto3\n",
    "!pip install configparser\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Setup OR Google Drive\n",
    "Run the cell below regardless of whether you are using google drive or local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally.\n"
     ]
    }
   ],
   "source": [
    "# if running locally set GOOGLE PATH\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n",
    "else:\n",
    "  GOOGLE_DRIVE_PATH = '.'\n",
    "  print('Running locally.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')  # Add the 'src' folder to Python's module search path\n",
    "sys.path.append('../datasets')  # Add the 'datasets' folder to Python's module search path\n",
    "sys.path.append('../notebooks')  # Add the 'notebooks' folder to Python's module search path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_utils import set_seed, MURADataset, get_transforms, load_data, confirm_images_and_labels, count_body_parts, count_positive_negative, count_body_parts_with_augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Seed\n",
    "\n",
    "This is so the results can be duplicated, ensure that the seed is set in the image_utils.py file, if you want a random seed, import random and set a random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGVUtueX353J"
   },
   "source": [
    "## Data Preprocessing\n",
    "Get the dataset, perform data augmentation to get finalized MURA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 728944,
     "status": "ok",
     "timestamp": 1731639512514,
     "user": {
      "displayName": "Karthik Subramanian",
      "userId": "06529395952522016450"
     },
     "user_tz": 300
    },
    "id": "sIU_T5hM3-YD",
    "outputId": "f7e6ae30-f070-4587-e02f-f09732ead1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to datasets\\MURA-v1.1.zip\n",
      "successfully unzipped the file at path c:\\code\\BoneawareAI\\datasets\\MURA-v1.1.zip\n"
     ]
    }
   ],
   "source": [
    "# Downloading MURA dataset and unzipping the file (this one takes time)\n",
    "from src.data_loader import download_dataset\n",
    "from src.constants import DATASETS_FOLDER, MURA_DATASET\n",
    "from src.helpers.utils import unzip_file\n",
    "download_dataset(MURA_DATASET, DATASETS_FOLDER)\n",
    "unzip_file(os.path.join(os.getcwd(), DATASETS_FOLDER, MURA_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 147232 validated image filenames belonging to 2 classes in the training set.\n",
      "Found 12788 validated image filenames belonging to 2 classes in the validation set.\n"
     ]
    }
   ],
   "source": [
    "#17 minutes to load local\n",
    "data_dir = \"../datasets/MURA-v1.1\"\n",
    "batch_size = 32\n",
    "\n",
    "# Load training and validation data\n",
    "train_loader, valid_loader = load_data(data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "Batch size: 32, Labels: tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1])\n",
      "Validation Data:\n",
      "Batch size: 32, Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data:\")\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Batch size: {len(images)}, Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "# Test the validation DataLoader\n",
    "print(\"Validation Data:\")\n",
    "for images, labels in valid_loader:\n",
    "    print(f\"Batch size: {len(images)}, Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training dataset: 147232\n",
      "Number of samples in the validation dataset: 12788\n"
     ]
    }
   ],
   "source": [
    "# Access the datasets from the DataLoaders\n",
    "train_dataset = train_loader.dataset\n",
    "valid_dataset = valid_loader.dataset\n",
    "\n",
    "# Example: Print the length of the datasets\n",
    "print(f\"Number of samples in the training dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in the validation dataset: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking train dataset...\n",
      "Total train images: 147232\n",
      "Unique labels in train dataset: [0, 1]\n",
      "\n",
      "Checking valid dataset...\n",
      "Total valid images: 12788\n",
      "Unique labels in valid dataset: [0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#16 minutes to confirm on local, does not need to run as you can always use the dataset to confirm as well\n",
    "confirm_images_and_labels(train_loader, \"train\")\n",
    "confirm_images_and_labels(valid_loader, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset body part distribution:\n",
      "XR_SHOULDER: 8379\n",
      "XR_HUMERUS: 1272\n",
      "XR_FINGER: 5106\n",
      "XR_ELBOW: 4931\n",
      "XR_WRIST: 9752\n",
      "XR_FOREARM: 1825\n",
      "XR_HAND: 5543\n",
      "\n",
      "Valid dataset body part distribution:\n",
      "XR_WRIST: 659\n",
      "XR_FOREARM: 301\n",
      "XR_HAND: 460\n",
      "XR_HUMERUS: 288\n",
      "XR_SHOULDER: 563\n",
      "XR_ELBOW: 465\n",
      "XR_FINGER: 461\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_body_parts(train_dataset, \"train\")\n",
    "count_body_parts(valid_dataset, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset body part distribution (with augmentations):\n",
      "XR_SHOULDER: Original: 8379, Augmented: 33516\n",
      "XR_HUMERUS: Original: 1272, Augmented: 5088\n",
      "XR_FINGER: Original: 5106, Augmented: 20424\n",
      "XR_ELBOW: Original: 4931, Augmented: 19724\n",
      "XR_WRIST: Original: 9752, Augmented: 39008\n",
      "XR_FOREARM: Original: 1825, Augmented: 7300\n",
      "XR_HAND: Original: 5543, Augmented: 22172\n",
      "\n",
      "Valid dataset body part distribution (with augmentations):\n",
      "XR_WRIST: Original: 659, Augmented: 2636\n",
      "XR_FOREARM: Original: 301, Augmented: 1204\n",
      "XR_HAND: Original: 460, Augmented: 1840\n",
      "XR_HUMERUS: Original: 288, Augmented: 1152\n",
      "XR_SHOULDER: Original: 563, Augmented: 2252\n",
      "XR_ELBOW: Original: 465, Augmented: 1860\n",
      "XR_FINGER: Original: 461, Augmented: 1844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with 3 augmentations,  adjust the augmentations as needed\n",
    "count_body_parts_with_augmentations(train_dataset, \"train\", num_augmentations=3)\n",
    "count_body_parts_with_augmentations(valid_dataset, \"valid\", num_augmentations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset positive/negative distribution (with augmentations):\n",
      "XR_ELBOW: Positive: 2006 (Augmented: 8024), Negative: 2925 (Augmented: 11700)\n",
      "XR_FINGER: Positive: 1968 (Augmented: 7872), Negative: 3138 (Augmented: 12552)\n",
      "XR_FOREARM: Positive: 661 (Augmented: 2644), Negative: 1164 (Augmented: 4656)\n",
      "XR_HAND: Positive: 1484 (Augmented: 5936), Negative: 4059 (Augmented: 16236)\n",
      "XR_HUMERUS: Positive: 599 (Augmented: 2396), Negative: 673 (Augmented: 2692)\n",
      "XR_SHOULDER: Positive: 4168 (Augmented: 16672), Negative: 4211 (Augmented: 16844)\n",
      "XR_WRIST: Positive: 3987 (Augmented: 15948), Negative: 5765 (Augmented: 23060)\n",
      "\n",
      "Valid dataset positive/negative distribution (with augmentations):\n",
      "XR_ELBOW: Positive: 230 (Augmented: 920), Negative: 235 (Augmented: 940)\n",
      "XR_FINGER: Positive: 247 (Augmented: 988), Negative: 214 (Augmented: 856)\n",
      "XR_FOREARM: Positive: 151 (Augmented: 604), Negative: 150 (Augmented: 600)\n",
      "XR_HAND: Positive: 189 (Augmented: 756), Negative: 271 (Augmented: 1084)\n",
      "XR_HUMERUS: Positive: 140 (Augmented: 560), Negative: 148 (Augmented: 592)\n",
      "XR_SHOULDER: Positive: 278 (Augmented: 1112), Negative: 285 (Augmented: 1140)\n",
      "XR_WRIST: Positive: 295 (Augmented: 1180), Negative: 364 (Augmented: 1456)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count positive/negative cases in the training dataset (with 3 augmentations)\n",
    "count_positive_negative(train_dataset, \"train\", num_augmentations=3)\n",
    "\n",
    "# Count positive/negative cases in the validation dataset (with 3 augmentations)\n",
    "count_positive_negative(valid_dataset, \"valid\", num_augmentations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "boneaware-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
