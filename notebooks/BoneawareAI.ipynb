{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CHWhyuzyfvZ"
   },
   "source": [
    "# BoneawareAI\n",
    "\n",
    "Authors: Karthik Subramanian, Charles Green, Sai Anurag Pichika, Saarang Prabhuram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFJ0ozK8yp3V"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0qyiSEXMe7v"
   },
   "source": [
    "### Load Extensions\n",
    "\n",
    "Before getting started we need to run some standard code to set up our environment. You'll need to execute this code again each time you start the notebook.\n",
    "\n",
    "First, run this cell to load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This enables us to modify `.py` source files and reintegrate them into the notebook, ensuring a smooth editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMh6Xv7hMe7w"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZpy-YIghhL_",
    "outputId": "4e767887-5b95-41ce-9180-1f480661900d"
   },
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pTbT7j1kM5Z",
    "outputId": "ff6c31e6-ee42-4b26-9bbd-244f8268a4ae"
   },
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sopx83pHMe7w"
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fFVquhuCVrq",
    "outputId": "c3d94a05-be93-455e-c722-a094f7d7b9c7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "It7_Rq3X4g3z",
    "outputId": "34600eb5-1758-4378-eff8-1364fd787abb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_PATH = 'BoneawareAI'\n",
    "GOOGLE_DRIVE_PATH = f'/content/drive/MyDrive/{PROJECT_PATH}'\n",
    "os.chdir(GOOGLE_DRIVE_PATH)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w70X5avjfR1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH) # this is important for the imports in the .py files to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkDj-6n-EMYa",
    "outputId": "3a53de82-04f3-4a2b-8653-da5db9c037a6"
   },
   "outputs": [],
   "source": [
    "%pip install pyyaml==5.4.1\n",
    "%pip install boto3\n",
    "%pip install configparser\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6-1yUtSMe7z"
   },
   "source": [
    "### Local Setup OR Google Drive\n",
    "Run the cell below regardless of whether you are using google drive or local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running locally set GOOGLE PATH\n",
    "import sys\n",
    "isLocal = False\n",
    "if 'google.colab' in sys.modules:\n",
    "  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n",
    "else:\n",
    "  GOOGLE_DRIVE_PATH = '.'\n",
    "  print('Running locally.')\n",
    "  isLocal = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CJzQJ5FMe7z"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHlDJ5n1Me7z",
    "outputId": "daabfc8a-88a9-4c75-dbbc-0350dbb52a2c"
   },
   "outputs": [],
   "source": [
    "# RUN LOCALLY\n",
    "import sys\n",
    "if isLocal:\n",
    "    sys.path.append('../src')  # Add the 'src' folder to Python's module search path\n",
    "    sys.path.append('../datasets')  # Add the 'datasets' folder to Python's module search path\n",
    "    sys.path.append('../notebooks')  # Add the 'notebooks' folder to Python's module search path\n",
    "    print('Modules added correctly, locally.')\n",
    "else:\n",
    "    sys.path.append('src')  # Add the 'src' folder to Python's module search path\n",
    "    sys.path.append('datasets')  # Add the 'datasets' folder to Python's module search path\n",
    "    sys.path.append('notebooks')  # Add the 'notebooks' folder to Python's module search path\n",
    "    print('Modules added correctly on colab.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ny2_zskeMe70"
   },
   "outputs": [],
   "source": [
    "from image_utils import set_seed, MURADataset, load_data, confirm_images_and_labels, count_body_parts, count_positive_negative, count_body_parts_with_augmentations, analyze_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import plot_confusion_matrix, plot_roc_curve, compute_class_weights, calculate_metrics, calculate_metrics_per_body_part, evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDEQTUPXMe70",
    "outputId": "6dc3334a-1fab-4794-ff86-af2eb15ce7ba"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device = \" + device)\n",
    "if device == 'cpu':\n",
    "    print(\"WARNING: Using CPU will cause slower train times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjzWXLZKMe70"
   },
   "source": [
    "#### Set Seed\n",
    "\n",
    "This is so the results can be duplicated, ensure that the seed is set in the image_utils.py file, if you want a random seed, import random and set a random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WpsZa4zMe70"
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iU--YN8_Me71"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGVUtueX353J"
   },
   "source": [
    "## Data Preprocessing\n",
    "Get the dataset, perform data augmentation to get finalized MURA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIU_T5hM3-YD",
    "outputId": "ee8e3161-12b7-4925-cec2-5e19be84d222"
   },
   "outputs": [],
   "source": [
    "# Downloading MURA dataset and unzipping the file (this one takes time)\n",
    "from data_loader import download_dataset\n",
    "from constants import DATASETS_FOLDER, MURA_DATASET\n",
    "from helpers.utils import unzip_file\n",
    "\n",
    "if (isLocal):\n",
    "    # Define the parent directory and dataset path\n",
    "    parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Go to the parent directory\n",
    "    datasets_folder = os.path.join(parent_dir, DATASETS_FOLDER)   # Define datasets folder in the parent directory\n",
    "    dataset_path = os.path.join(datasets_folder, MURA_DATASET)    # Full path to the dataset file\n",
    "else:\n",
    "    datasets_folder = os.path.join(GOOGLE_DRIVE_PATH, DATASETS_FOLDER) # Define datasets folder in the parent directory\n",
    "    dataset_path = os.path.join(datasets_folder, MURA_DATASET) # Full path to the dataset file\n",
    "\n",
    "# Ensure the datasets folder exists\n",
    "os.makedirs(datasets_folder, exist_ok=True)\n",
    "\n",
    "# Check if the dataset is already downloaded\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"{MURA_DATASET} not found in {DATASETS_FOLDER}. Downloading and extracting...\")\n",
    "    # Download and unzip the dataset\n",
    "    download_dataset(MURA_DATASET, datasets_folder)\n",
    "    unzip_file(dataset_path)\n",
    "else:\n",
    "    print(f\"{MURA_DATASET} already exists in {DATASETS_FOLDER}. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.model_utils import get_hyperparameters\n",
    "\n",
    "# Retrieving all hyper parameters\n",
    "lr, weight_decay, num_epochs, step_size, gamma, batch_size, factor, patience = get_hyperparameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1BtlQ_dMe71",
    "outputId": "fff83bf5-2616-4228-e906-f9804619a3ad"
   },
   "outputs": [],
   "source": [
    "if (isLocal):\n",
    "    data_dir = \"../datasets/MURA-v1.1\"\n",
    "else:\n",
    "    data_dir = os.path.join(datasets_folder, 'MURA-v1.1')\n",
    "\n",
    "# Load training and validation data\n",
    "train_loader, valid_loader = load_data(data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQ8zvhHiMe71",
    "outputId": "4c03b4ee-de8a-4456-da85-e33db4380770"
   },
   "outputs": [],
   "source": [
    "print(\"Training Data:\")\n",
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "    print(f\"Batch size: {len(images)}, Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "# Test the validation DataLoader\n",
    "print(\"Validation Data:\")\n",
    "for images, labels in valid_loader:\n",
    "    print(f\"Batch size: {len(images)}, Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dxz7Q2vMe72",
    "outputId": "92524caf-3762-47db-c34f-460ed94fe27c"
   },
   "outputs": [],
   "source": [
    "# Access the datasets from the DataLoaders\n",
    "train_dataset = train_loader.dataset\n",
    "valid_dataset = valid_loader.dataset\n",
    "\n",
    "# Example: Print the length of the datasets\n",
    "print(f\"Number of samples in the training dataset: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in the validation dataset: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MACZwlyXMe72"
   },
   "outputs": [],
   "source": [
    "#16 minutes to confirm on local, does not need to run as you can always use the dataset to confirm as well\n",
    "#confirm_images_and_labels(train_dataset, \"train\")\n",
    "#confirm_images_and_labels(valid_dataset, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "wKC-U9oAMe72",
    "outputId": "efadada7-3521-4e98-d1d6-7492f295173e"
   },
   "outputs": [],
   "source": [
    "count_body_parts(train_dataset, \"train\")\n",
    "count_body_parts(valid_dataset, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "2LsFOtliMe72",
    "outputId": "b738cf38-e421-4cf4-8673-7624d20967df"
   },
   "outputs": [],
   "source": [
    "# Example usage with 3 augmentations,  adjust the augmentations as needed\n",
    "count_body_parts_with_augmentations(train_dataset, \"train\", num_augmentations=3)\n",
    "count_body_parts_with_augmentations(valid_dataset, \"valid\", num_augmentations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "h03i-7tEMe72",
    "outputId": "2426fb6c-31ed-4c13-f2af-f390f6198484"
   },
   "outputs": [],
   "source": [
    "# Count positive/negative cases in the training dataset (with 3 augmentations)\n",
    "count_positive_negative(train_dataset, \"train\", num_augmentations=3)\n",
    "\n",
    "# Count positive/negative cases in the validation dataset (with 3 augmentations)\n",
    "count_positive_negative(valid_dataset, \"valid\", num_augmentations=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgQmrLwHMe73"
   },
   "source": [
    "### Other Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ju8EWFkaMe73"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlh28zGVMe73"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmACXssNX7MM"
   },
   "source": [
    "### DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH0qm718X_zy"
   },
   "outputs": [],
   "source": [
    "from helpers.model_utils import get_model\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the Model\n",
    "model_name = \"vgg\" # \"densenet\", \"resnet\", \"vgg\", \"custom_cnn1\"\n",
    "model = get_model(model_name, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Dynamically compute class weights\\n\",\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"\n",
    "    Computes class weights efficiently for binary classification.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The training dataset.\n",
    "\n",
    "    Returns:\\n\",\n",
    "        torch.Tensor: Class weights as a tensor.\n",
    "    \"\"\"\n",
    "    # Extract labels from label_map\n",
    "    labels = list(dataset.label_map.values())\n",
    "\n",
    "    # Use scikit-learn to compute balanced class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "\n",
    "    # Convert weights to a PyTorch tensor\n",
    "    return torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c02N-mzEYAVe"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 6. Define Loss, Optimizer, and Scheduler\n",
    "# Compute class weights dynamically\n",
    "weights = compute_class_weights(train_dataset).to(device)\n",
    "# Define loss function with dynamic weights\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights[1])\n",
    "# Define optimizer with adjusted weight decay\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# Use ReduceLROnPlateau for adaptive learning rate adjustment\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Computed Class Weights:\", {weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_auc_score, classification_report,\n",
    "    precision_recall_fscore_support, cohen_kappa_score, roc_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfL-ipYSYYa-",
    "outputId": "d62a9554-953f-4bd6-c41d-8fe76f187eae"
   },
   "outputs": [],
   "source": [
    "from helpers.trainer import train_model\n",
    "# 8. Train the Model\n",
    "model, train_history = train_model(model, criterion, optimizer, scheduler, train_loader, valid_loader, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opU1Rx49Yajn"
   },
   "outputs": [],
   "source": [
    "model_name = \"Custom_VGG13_BN.pth\"\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), f'{model_name}')\n",
    "# Load the model\n",
    "if device == 'cuda':\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "elif device == 'mps':\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device('mps')))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_name, map_location=torch.device('cpu')))\n",
    "# Evaluate on validation set\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of parameters and model size\n",
    "models = [\"densenet_mura.pth\"] \n",
    "results = analyze_models(models)\n",
    "\n",
    "for model_path, details in results.items():\n",
    "    print(f\"File: {model_path}\")\n",
    "    print(f\"  Number of parameters: {details['num_parameters']}\")\n",
    "    print(f\"  Model size: {details['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "metrics = evaluate_model(model, valid_loader, dataset=valid_dataset, criterion=criterion)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualizer import find_last_conv_layer, run_gradcam, GradCAM, run_gradcam_filtered, run_gradcam_for_path_person_or_bodypart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name, target_layer = find_last_conv_layer(model)\n",
    "print(layer_name)\n",
    "print(target_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates minimum n images or batch size images from first batch\n",
    "run_gradcam(model, valid_loader, target_layer, class_names=[\"Normal\", \"Abnormal\"], device=device, num_images=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = [\"XR_ELBOW\", \"XR_FINGER\", \"XR_FOREARM\", \"XR_HAND\", \"XR_HUMERUS\", \"XR_SHOULDER\", \"XR_WRIST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 3 images from each bodypart\n",
    "for x in body_parts:\n",
    "    run_gradcam_filtered(\n",
    "        model=model,\n",
    "        dataloader=valid_loader,\n",
    "        target_layer=target_layer,\n",
    "        class_names=[\"Normal\", \"Abnormal\"],\n",
    "        body_part=x,\n",
    "        n=1,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific Image Path, must have these slashes /\n",
    "run_gradcam_for_path_person_or_bodypart(\n",
    "    model=model,\n",
    "    dataloader=valid_loader,\n",
    "    target_layer=target_layer,\n",
    "    class_names=[\"Normal\", \"Abnormal\"],\n",
    "    image_path=\"MURA-v1.1/valid/XR_ELBOW/patient11186/study1_positive/image1.png\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific Body Part on a Person\n",
    "run_gradcam_for_path_person_or_bodypart(\n",
    "    model=model,\n",
    "    dataloader=valid_loader,\n",
    "    target_layer=target_layer,\n",
    "    class_names=[\"Normal\", \"Abnormal\"],\n",
    "    person_id=\"patient11185\",\n",
    "    body_part=\"XR_WRIST\",\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific Person\n",
    "run_gradcam_for_path_person_or_bodypart(\n",
    "    model=model,\n",
    "    dataloader=valid_loader,\n",
    "    target_layer=target_layer,\n",
    "    class_names=[\"Normal\", \"Abnormal\"],\n",
    "    person_id=\"patient11188\",\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "boneaware-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
